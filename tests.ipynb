{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "median-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os, argparse\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data.distributed\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import horovod.torch as hvd\n",
    "import tensorboardX\n",
    "\n",
    "from models import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "optimum-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--lr LR] [--resume]\n",
      "                             [--checkpoint-format CHECKPOINT_FORMAT]\n",
      "                             [--log-dir LOG_DIR] [--fp16-allreduce]\n",
      "                             [--batches-per-allreduce BATCHES_PER_ALLREDUCE]\n",
      "                             [--batch-size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--warmup-epochs WARMUP_EPOCHS]\n",
      "                             [--momentum MOMENTUM]\n",
      "                             [--weight-decay WEIGHT_DECAY] [--seed SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/cdsw/.local/share/jupyter/runtime/kernel-b3fbba43-4968-469f-9dbe-219a51d5542f.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Horovod/PyTorch CIFAR10 Training')\n",
    "parser.add_argument('--lr', default=0.0125, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "parser.add_argument('--checkpoint-format', default='./checkpoint-{epoch}.pth.tar',\n",
    "                    help='checkpoint file format')\n",
    "parser.add_argument('--log-dir', default='./logs',\n",
    "                    help='tensorboard log directory')\n",
    "parser.add_argument('--fp16-allreduce', action='store_true', default=False,\n",
    "                    help='use fp16 compression during allreduce')\n",
    "parser.add_argument('--batches-per-allreduce', type=int, default=1,\n",
    "                    help='number of batches processed locally before '\n",
    "                         'executing allreduce across workers; it multiplies '\n",
    "                         'total batch size.')\n",
    "parser.add_argument('--batch-size', type=int, default=128,\n",
    "                    help='input batch size for training')\n",
    "parser.add_argument('--epochs', type=int, default=90,\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--warmup-epochs', type=float, default=5,\n",
    "                    help='number of warmup epochs')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='SGD momentum')\n",
    "parser.add_argument('--weight-decay', type=float, default=5e-4,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--seed', type=int, default=42,\n",
    "                    help='random seed')\n",
    "args = parser.parse_args()\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "progressive-clinic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Horovod ...\n",
      "Using cpu.\n"
     ]
    }
   ],
   "source": [
    "os.system('init-hvd.sh')\n",
    "print('Initializing Horovod ...')\n",
    "hvd.init()\n",
    "device = 'cpu'\n",
    "print('Using %s.' % device)\n",
    "allreduce_batch_size = 10 * 10\n",
    "\n",
    "# Horovod: broadcast resume_from_epoch from rank 0 (which will have\n",
    "# checkpoints) to other ranks.\n",
    "resume_from_epoch = hvd.broadcast(torch.tensor(3), root_rank=0, name='resume_from_epoch').item()\n",
    "# Horovod: print logs on the first worker.\n",
    "verbose = 1 if hvd.rank() == 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-implementation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-geology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-wheat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-climb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "exceptional-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "scheduled-jaguar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data_new', train=True, download=True, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "white-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.distributed.DistributedSampler(trainset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "subtle-addiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data_new\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomCrop(size=(32, 32), padding=4)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
       "           )"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "capital-protocol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_sampler = torch.utils.data.distributed.DistributedSampler(testset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, sampler=test_sampler)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "thrown-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "changing-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "driving-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Horovod: scale learning rate by the number of GPUs.\n",
    "# Gradient Accumulation: scale learning rate by batches_per_allreduce\n",
    "optimizer = optim.SGD(net.parameters(), lr=(2 * 2 * 100), \n",
    "                                    momentum=1, weight_decay=1)\n",
    "# Horovod: (optional) compression algorithm.\n",
    "compression = hvd.Compression.fp16\n",
    "# Horovod: wrap optimizer with DistributedOptimizer.\n",
    "optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=net.named_parameters(),\n",
    "                                    compression=compression, backward_passes_per_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "meaning-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horovod: broadcast parameters & optimizer state.\n",
    "hvd.broadcast_parameters(net.state_dict(), root_rank=0)\n",
    "hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-belief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch     #2:   6%|▌         | 2902/50000 [03:52<50:15, 15.62it/s, loss=nan, accuracy=9.51]    "
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % 1)\n",
    "    net.train()\n",
    "    train_sampler.set_epoch(1)\n",
    "    train_loss = Metric('train_loss')\n",
    "    train_accuracy = Metric('train_accuracy')\n",
    "\n",
    "    with tqdm(total=len(trainloader), desc='Train Epoch     #{}'.format(epoch + 1), disable=not verbose) as t:\n",
    "        for batch_idx, (data, target) in enumerate(trainloader):\n",
    "            adjust_learning_rate(epoch, batch_idx)\n",
    "\n",
    "            if device=='cuda':\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            # Split data into sub-batches of size batch_size\n",
    "            for i in range(0, len(data), 1):\n",
    "                data_batch = data[i:i + 1]\n",
    "                target_batch = target[i:i + 1]\n",
    "                output = net(data_batch)\n",
    "                train_accuracy.update(accuracy(output, target_batch))\n",
    "                loss = F.cross_entropy(output, target_batch)\n",
    "                train_loss.update(loss)\n",
    "                # Average gradients among sub-batches\n",
    "                loss.div_(math.ceil(float(len(data)) / 1))\n",
    "                loss.backward()\n",
    "            # Gradient is applied across all ranks\n",
    "            optimizer.step()\n",
    "            t.set_postfix({'loss': train_loss.avg.item(), 'accuracy': 100. * train_accuracy.avg.item()})\n",
    "            t.update(1)\n",
    "\n",
    "    if log_writer:\n",
    "        log_writer.add_scalar('train/loss', train_loss.avg, epoch)\n",
    "        log_writer.add_scalar('train/accuracy', train_accuracy.avg, epoch)\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(total=len(testloader), desc='Validate Epoch  #{}'.format(epoch + 1), disable=not verbose) as t:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                t.set_postfix({'loss': test_loss.avg.item(), 'accuracy': 100. * test_accuracy.avg.item()})\n",
    "                t.update(1)\n",
    "\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    net.eval()\n",
    "    test_loss = Metric('test_loss')\n",
    "    test_accuracy = Metric('test_accuracy')\n",
    "\n",
    "    with tqdm(total=len(testloader),\n",
    "              desc='Validate Epoch  #{}'.format(epoch + 1),\n",
    "              disable=not verbose) as t:\n",
    "        with torch.no_grad():\n",
    "            for data, target in testloader:\n",
    "                if device=='cuda':\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                output = net(data)\n",
    "\n",
    "                test_loss.update(F.cross_entropy(output, target))\n",
    "                test_accuracy.update(accuracy(output, target))\n",
    "                t.set_postfix({'loss': test_loss.avg.item(),\n",
    "                               'accuracy': 100. * test_accuracy.avg.item()})\n",
    "                t.update(1)\n",
    "\n",
    "    if log_writer:\n",
    "        log_writer.add_scalar('test/loss', test_loss.avg, epoch)\n",
    "        log_writer.add_scalar('test/accuracy', test_accuracy.avg, epoch)\n",
    "\n",
    "\n",
    "# Horovod: using `lr = base_lr * hvd.size()` from the very beginning leads to worse final\n",
    "# accuracy. Scale the learning rate `lr = base_lr` ---> `lr = base_lr * hvd.size()` during\n",
    "# the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n",
    "# After the warmup reduce learning rate by 10 on the 30th, 60th and 80th epochs.\n",
    "def adjust_learning_rate(epoch, batch_idx):\n",
    "    if epoch < 3:\n",
    "        epoch += float(batch_idx + 1) / len(trainloader)\n",
    "        lr_adj = 1. / 1 * (epoch * (2 - 1) / 2 + 1)\n",
    "    elif epoch < 30:\n",
    "        lr_adj = 1.\n",
    "    elif epoch < 60:\n",
    "        lr_adj = 1e-1\n",
    "    elif epoch < 80:\n",
    "        lr_adj = 1e-2\n",
    "    else:\n",
    "        lr_adj = 1e-3\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 23 * 3 * 2 * lr_adj\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    # get the index of the max log-probability\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    return pred.eq(target.view_as(pred)).cpu().float().mean()\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch):\n",
    "    if hvd.rank() == 0:\n",
    "        filepath = 'home/cdsw/'\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state, filepath)\n",
    "\n",
    "\n",
    "# Horovod: average metrics from distributed training.\n",
    "class Metric(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sum = torch.tensor(0.)\n",
    "        self.n = torch.tensor(0.)\n",
    "\n",
    "    def update(self, val):\n",
    "        self.sum += hvd.allreduce(val.detach().cpu(), name=self.name)\n",
    "        self.n += 1\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / self.n\n",
    "\n",
    "\n",
    "for epoch in range(1, 1+200):\n",
    "    train(epoch)\n",
    "    test(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-television",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
